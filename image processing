import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from tensorflow.keras.preprocessing import image

# 1. Define your data directories
train_dir = 'data/train'   # should contain 'defective/' and 'non_defective/' subfolders
test_dir  = 'data/test'    # same structure as train_dir

# 2. Create ImageDataGenerators to load & normalize images
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen  = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary'    # binary labels: defective (1) or non_defective (0)
)

test_gen = test_datagen.flow_from_directory(
    test_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary'
)

# 3. Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),                             #relu so after scanning the image, the nucleus will output numbers, that number will have negative and positive. Relu will keep positive numbers and convert negative to 0
    MaxPooling2D((2, 2)),                                                                         #maxPooling will make the image smaller but still enhancing the defected images

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    Flatten(),                                                        # flatten feature maps to a 1D vector
    Dense(128, activation='relu'),                                     #Dense will create another layer of nucleus to connect to the 2048 nucleus. densewill be 168 nucleus
    Dropout(0.5),                                                     # prevent overfitting
    Dense(1, activation='sigmoid')# output layer for binary classification
])

# 4. Compile the model
model.compile(
    optimizer='adam',                 #Adam makes training faster and often better without much manual tuning
                                    #Think of it like cruise control in a car: You still steer and set the speed (your hyperparameters),
                                    #But Adam automatically adjusts the engine (learning) so the ride is smoother and faster.
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# 5. Train the model
model.fit(
    train_gen,
    epochs=10, if its  100 photos. it will scan it again 10 times
    validation_data=test_gen
)

# 6. Evaluate on the test set
loss, accuracy = model.evaluate(test_gen)
print(f"Test accuracy: {accuracy * 100:.2f}%")

# 7. Predict on a single new image
img_path = 'data/test/defective/example.jpg'  # replace with your image path
img = image.load_img(img_path, target_size=(64, 64))
img_arr = image.img_to_array(img) / 255.0      # normalize same as training
img_arr = np.expand_dims(img_arr, axis=0)      # add batch dimension

prediction = model.predict(img_arr)[0][0]
if prediction > 0.5:
    print("Defective")
else:
    print("Non-defective")
